{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as distance\n",
    "from gensim import corpora, models, similarities, matutils, utils\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (similarity, document1, document2)\n",
    "documents = []\n",
    "\n",
    "path = os.path.join('stsbenchmark', 'sts-train.csv')\n",
    "\n",
    "with open(path, 'r') as f:\n",
    "    for line in list(f):\n",
    "        parts = line.strip().split('\\t')\n",
    "        \n",
    "        sim = float(parts[4])\n",
    "        doc1 = list(utils.simple_tokenize(parts[5]))\n",
    "        doc2 = list(utils.simple_tokenize(parts[6]))\n",
    "\n",
    "        documents.append((sim, doc1, doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.0, ['A', 'plane', 'is', 'taking', 'off'], ['An', 'air', 'plane', 'is', 'taking', 'off'])\n",
      "(3.8, ['A', 'man', 'is', 'playing', 'a', 'large', 'flute'], ['A', 'man', 'is', 'playing', 'a', 'flute'])\n",
      "(3.8, ['A', 'man', 'is', 'spreading', 'shreded', 'cheese', 'on', 'a', 'pizza'], ['A', 'man', 'is', 'spreading', 'shredded', 'cheese', 'on', 'an', 'uncooked', 'pizza'])\n",
      "(2.6, ['Three', 'men', 'are', 'playing', 'chess'], ['Two', 'men', 'are', 'playing', 'chess'])\n",
      "(4.25, ['A', 'man', 'is', 'playing', 'the', 'cello'], ['A', 'man', 'seated', 'is', 'playing', 'the', 'cello'])\n"
     ]
    }
   ],
   "source": [
    "for triple in documents[:5]:\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims, docs1, docs2 = zip(*documents)\n",
    "\n",
    "docs = docs1 + docs2 # list of all documents\n",
    "\n",
    "dictionary = corpora.Dictionary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12955 unique tokens: ['A', 'is', 'off', 'plane', 'taking']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vector representation of documents\n",
    "corpus1 = [dictionary.doc2bow(doc) for doc in docs1]\n",
    "corpus2 = [dictionary.doc2bow(doc) for doc in docs2]\n",
    "\n",
    "common_corpus = corpus1 + corpus2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(0, 1), (1, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(0, 1), (1, 1), (5, 1), (8, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1)], [(9, 1), (15, 1), (16, 1), (17, 1), (18, 1)], [(0, 1), (1, 1), (8, 1), (9, 1), (19, 1), (20, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# corpus is a list of sparse vectors\n",
    "print(common_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_similarities(model, corpus1, corpus2):\n",
    "    '''Compare corpus1[i] to corpus2[i] for all i \n",
    "    and return the cosine similarity values'''\n",
    "    return [matutils.cossim(model[corpus1[i]], model[corpus2[i]])\n",
    "            for i in range(len(corpus1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correl(pred, real):\n",
    "    '''Returns pearson correlation coef of pred and real'''\n",
    "    return np.corrcoef(pred, real)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Frequency * Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf can be seen as a modified version of term-document matrix which takes into account the frequency of the words. Words that occur in smaller amount of documents are likely to reveal more information than common words (such as stopwords) and therefore higher weight is assigned to them. Tf-idf value for a word is $\\textbf{term frequency} \\times \\textbf{inverse document frequency}$\n",
    "\n",
    "For word $i$ in document $j$ $\\text{tf}_{ij}$ can be read from the term-document matrix and $\\text{idf}_{i} = \\log \\left( \\frac{N}{df_i} \\right)$ where $N$ is the total number of documents and $\\text{df}_i$ is the amount of documents where word $i$ occurs.\n",
    "\n",
    "Combining these results in tf-idf matrix where weight for word $i$ in document $j$ is $w_{ij} = \\text{tf}_{ij} \\times \\text{idf}_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information: Chapter 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(common_corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6228996261601334"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_pred = predict_similarities(tfidf, corpus1, corpus2)\n",
    "correl(tfidf_pred, sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSI is based on SVD which is one way to factorize a matrix. When SVD is applied to the term-document matrix it is factorized into $W \\times \\Sigma \\times C^T$ where rows of $W$ represent words in latent space and columns are ordered w.r.t importance (singular values), $\\Sigma$ is a diagonal matrix that contains the singular values indicating importance and $C^T$ is a matrix representing the documents.\n",
    "\n",
    "Since the most important details are captured by the columns corresponding to the largest singular values, a dense vector representation can be obtained by truncating matrix $W$ from $|V| \\times m$ to $|V| \\times k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information: Chaper 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = models.LsiModel(common_corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2650836598138713"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_pred = predict_similarities(lsi, corpus1, corpus2)\n",
    "correl(lsi_pred, sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition: In RP model the original data is projected into k-dimensional subspace using a random matrix $R$:\n",
    "\n",
    "$X^{RP}_{k \\times N} = R_{k \\times d} X_{d \\times N}$\n",
    "\n",
    "Johnson-Lindenstrauss lemma: distances are approximately preserved if points in vector space are projected onto a randomly selected subspace of suitably high dimension\n",
    "\n",
    "Strictly speaking this is not a projection since $R$ has not chosen to be orthogonal. However, there exists much larger amount of almost orthogonal than orthogonal directions and therefore random directions are sufficiently close to orthogonal directions.\n",
    "\n",
    "Simple way to initialize $R$ is\n",
    "\n",
    "$r _ { i j } = \\sqrt { 3} \\cdot \\left\\{ \\begin{array} { l l } { + 1} & { \\text{ with probability } \\frac { 1} { 6} } \\\\ { 0} & { \\text{ with probability } \\frac { 2} { 3} } \\\\ { - 1} & { \\text{ with probability } \\frac { 1 } { 6} } \\end{array} \\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information: https://users.ics.aalto.fi/ella/publications/randproj_kdd.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rp = models.RpModel(common_corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47392675093096365"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rp_pred = predict_similarities(rp, corpus1, corpus2)\n",
    "correl(rp_pred, sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a probabilistic extension of LSA. More information: https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    }
   ],
   "source": [
    "lda = models.LdaModel(common_corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3248790289498413"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_pred = predict_similarities(lda, corpus1, corpus2)\n",
    "correl(lda_pred, sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchial Dirichlet Process (HDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDP is a non-parametric bayesian method. More information: http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdp = models.HdpModel(common_corpus, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03723236282081314"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdp_pred = predict_similarities(hdp, corpus1, corpus2)\n",
    "correl(hdp_pred, sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogEntropy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maps BoW representation into log entropy space. \n",
    "\n",
    "More information:\n",
    "\n",
    "https://radimrehurek.com/gensim/models/logentropy_model.html\n",
    "\n",
    "https://stats.stackexchange.com/questions/215418/difference-between-log-entropy-model-and-tf-idf-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logentropy = models.LogEntropyModel(common_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6233722042921176"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logentropy_pred = predict_similarities(logentropy, corpus1, corpus2)\n",
    "correl(logentropy_pred, sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean of word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A document can be transformed by using the mean of the transformed words.\n",
    "\n",
    "More information:\n",
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec = models.Word2Vec(docs, min_count=1, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06897800962297461"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_pred = []\n",
    "\n",
    "for i in range(len(docs1)):\n",
    "\n",
    "    # mean of word2vecs for doc1 and doc2\n",
    "    doc1 = np.mean([word2vec.wv[word] for word in docs1[i]], axis=0)\n",
    "    doc2 = np.mean([word2vec.wv[word] for word in docs2[i]], axis=0)\n",
    "    \n",
    "    # doc1 and doc2 are dense vectors so let's use cosine distance from scipy\n",
    "    #\n",
    "    # cossim = 1 - cosdist\n",
    "    cossim = 1 - distance.cosine(doc1, doc2)\n",
    "    \n",
    "    word2vec_pred.append(cossim)\n",
    "    \n",
    "correl(word2vec_pred, sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec is a generalizatoin of Word2Vec which makes it possible to transform whole documents into a fixed-size vectors. Authors of Doc2Vec paper claim it to overcome the weaknesses of BoW representation: loss of word order and ignoring the semantics.\n",
    "\n",
    "More information:\n",
    "\n",
    "https://cs.stanford.edu/~quocle/paragraph_vector.pdf\n",
    "\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# docs into list of TaggedDocuments\n",
    "tagged_docs = [TaggedDocument(docs[i], [i]) for i in range(len(docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['A', 'plane', 'is', 'taking', 'off'], tags=[0]),\n",
       " TaggedDocument(words=['A', 'man', 'is', 'playing', 'a', 'large', 'flute'], tags=[1]),\n",
       " TaggedDocument(words=['A', 'man', 'is', 'spreading', 'shreded', 'cheese', 'on', 'a', 'pizza'], tags=[2]),\n",
       " TaggedDocument(words=['Three', 'men', 'are', 'playing', 'chess'], tags=[3]),\n",
       " TaggedDocument(words=['A', 'man', 'is', 'playing', 'the', 'cello'], tags=[4]),\n",
       " TaggedDocument(words=['Some', 'men', 'are', 'fighting'], tags=[5]),\n",
       " TaggedDocument(words=['A', 'man', 'is', 'smoking'], tags=[6]),\n",
       " TaggedDocument(words=['The', 'man', 'is', 'playing', 'the', 'piano'], tags=[7]),\n",
       " TaggedDocument(words=['A', 'man', 'is', 'playing', 'on', 'a', 'guitar', 'and', 'singing'], tags=[8]),\n",
       " TaggedDocument(words=['A', 'person', 'is', 'throwing', 'a', 'cat', 'on', 'to', 'the', 'ceiling'], tags=[9])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2vec = models.Doc2Vec(tagged_docs, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18285143797097164"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_pred = []\n",
    "\n",
    "for i in range(len(docs1)):\n",
    "    doc1 = doc2vec.infer_vector(docs1[i])\n",
    "    doc2 = doc2vec.infer_vector(docs2[i])\n",
    "    \n",
    "    cossim = 1 - distance.cosine(doc1, doc2)\n",
    "    \n",
    "    doc2vec_pred.append(cossim)\n",
    "    \n",
    "correl(doc2vec_pred, sims)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
